{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../'  # '/media/xiaoxy/2018-Kaggle-AdTrackingFraud/'\n",
    "predictors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Helper function ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(df, column_name):\n",
    "    df_onehot = pd.get_dummies(df[column_name], prefix=column_name)\n",
    "    df_all = pd.concat([df.drop([column_name], axis=1), df_onehot], axis=1)\n",
    "    predictors.append(column_name)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def encode_count(df, column_name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(list(df[column_name].values))\n",
    "    df[column_name] = le.transform(list(df[column_name].values))\n",
    "    predictors.append(column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_count(df, columns_groupby, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby).size()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_nunique(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].nunique()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_cumcount(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    df[new_column_name] = df.groupby(columns_groupby)[column].cumcount().values.astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_median(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].median()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_mean(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].mean()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_sum(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].sum()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    # predictors.append(new_column_name)  # bug: twice\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_max(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].max()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_min(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].min()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_std(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].std()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_var(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].var()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_rank(df, columns_groupby, column, new_column_name, ascending=True, type='uint64'):\n",
    "    df[new_column_name] = df.groupby(columns_groupby)[column].rank(ascending=ascending)\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_count(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feat.groupby(columns_groupby)[column].count()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_count.columns = columns_groupby + [column + \"_gb_%s_count\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_count.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_count, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_count.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_nunique(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_nunique = pd.DataFrame(df_feat.groupby(columns_groupby)[column].nunique()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_nunique.columns = columns_groupby + [column + \"_%s_nunique\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_nunique.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_nunique, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_nunique.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_mean(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_mean = pd.DataFrame(df_feat.groupby(columns_groupby)[column].mean()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_mean.columns = columns_groupby + [column + \"_%s_mean\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_mean.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_mean, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_mean.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_std(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_std = pd.DataFrame(df_feat.groupby(columns_groupby)[column].std()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_std.columns = columns_groupby + [column + \"_%s_std\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_std.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_std, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_std.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_median(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_median = pd.DataFrame(df_feat.groupby(columns_groupby)[column].median()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_median.columns = columns_groupby + [column + \"_%s_median\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_median.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_median, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_median.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_max(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_max = pd.DataFrame(df_feat.groupby(columns_groupby)[column].max()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_max.columns = columns_groupby + [column + \"_%s_max\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_max.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_max, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_max.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_min(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_min = pd.DataFrame(df_feat.groupby(columns_groupby)[column].min()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_min.columns = columns_groupby + [column + \"_%s_min\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_min.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_min, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_min.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_sum(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_sum = pd.DataFrame(df_feat.groupby(columns_groupby)[column].sum()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_sum.columns = columns_groupby + [column + \"_%s_sum\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_sum.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_sum, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_sum.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_var(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_var = pd.DataFrame(df_feat.groupby(columns_groupby)[column].var()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_var.columns = columns_groupby + [column + \"_%s_var\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_var.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_var, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_var.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_quantile(df, df_feat, columns_groupby, column, quantile_n, new_column_name=\"\"):\n",
    "    df_quantile = pd.DataFrame(df_feat.groupby(columns_groupby)[column].quantile(quantile_n)).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_quantile.columns = columns_groupby + [column + \"_%s_quantile\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_quantile.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_quantile, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_quantile.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_skew(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_skew = pd.DataFrame(df_feat.groupby(columns_groupby)[column].skew()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_skew.columns = columns_groupby + [column + \"_%s_skew\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_skew.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_skew, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_skew.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_rank_sp(df, feat1, feat2, ascending):\n",
    "    df.sort_values([feat1, feat2], inplace=True, ascending=ascending)\n",
    "    df['rank'] = range(df.shape[0])\n",
    "    min_rank = df.groupby(feat1, as_index=False)['rank'].agg({'min_rank': 'min'})\n",
    "    df = pd.merge(df, min_rank, on=feat1, how='left')\n",
    "    df['rank'] = df['rank'] - df['min_rank']\n",
    "    predictors.append('rank')\n",
    "    del df['min_rank']\n",
    "    return df\n",
    "\n",
    "\n",
    "def log(info):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' ' + str(info))\n",
    "\n",
    "\n",
    "def log_shape(train, test):\n",
    "    log('Train data shape: %s' % str(train.shape))\n",
    "    log('Test data shape: %s' % str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(df):\n",
    "    format = '%Y-%m-%d %H:%M:%S'\n",
    "    df['date'] = pd.to_datetime(df['click_time'], format=format)\n",
    "    df['month'] = df['date'].dt.month.astype('uint8')\n",
    "    df['weekday'] = df['date'].dt.weekday.astype('uint8')\n",
    "    df['day'] = df['date'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['date'].dt.hour.astype('uint8')\n",
    "    df['minute'] = df['date'].dt.minute.astype('uint8')\n",
    "    df['second'] = df['date'].dt.second.astype('uint8')\n",
    "    df['tm_hour'] = (df['hour'] + df['minute'] / 60.0).astype('float32')\n",
    "    df['tm_hour_sin'] = (df['tm_hour'].map(lambda x: math.sin((x - 12) / 24 * 2 * math.pi))).astype('float32')\n",
    "    df['tm_hour_cos'] = (df['tm_hour'].map(lambda x: math.cos((x - 12) / 24 * 2 * math.pi))).astype('float32')\n",
    "    del df['click_time']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Construct features function - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_next_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['ip', 'app', 'channel', 'device', 'os']},\n",
    "        {'columns': ['ip', 'os', 'device']},\n",
    "        {'columns': ['ip', 'os', 'device', 'app']}\n",
    "    ]\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df[all_features].groupby(spec['columns']).date.shift(-1) - df.date).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cal_prev_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['ip', 'channel']},\n",
    "        {'columns': ['ip', 'os']}\n",
    "    ]\n",
    "    # Calculate the time to prev click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df.date - df[all_features].groupby(spec['columns']).date.shift(+1)).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cal_cvr(train, test, type='float32'):\n",
    "    train['cvr_gb_ip_day_hour'] = 0\n",
    "    train['cvr_gb_ip_app'] = 0\n",
    "    train['cvr_gb_ip_app_os'] = 0\n",
    "\n",
    "    # Define group by list\n",
    "    idh = ['ip', 'day', 'hour']\n",
    "    ia = ['ip', 'app']\n",
    "    iao = ['ip', 'app', 'os']\n",
    "\n",
    "#     kf = KFold(train.shape[0], n_splits=5, shuffle=True, random_state=7)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "        log('Fold ' + str(i) + ' begin...')\n",
    "\n",
    "        # Divide train/test fold\n",
    "        tr = train.iloc[train_index, :train.shape[1] - 3]\n",
    "        te = train.iloc[test_index, :train.shape[1] - 3]\n",
    "\n",
    "        # Calculate sum of label of train folds\n",
    "        log('Cal sum_label_gb_ip_day_hour')\n",
    "        tr = merge_sum(tr, idh, 'is_attributed', 'sum_label_gb_ip_day_hour')\n",
    "        log('Cal sum_label_gb_ip_app')\n",
    "        tr = merge_sum(tr, ia, 'is_attributed', 'sum_label_gb_ip_app')\n",
    "        log('Cal sum_label_gb_ip_app_os')\n",
    "        tr = merge_sum(tr, iao, 'is_attributed', 'sum_label_gb_ip_app_os')\n",
    "\n",
    "        # Calculate cvr of train folds with using smothing technique\n",
    "        tr['cvr_gb_ip_day_hour'] = GaussianSmoth().update_moment(tr['count_gb_ip_day_hour'], tr['sum_label_gb_ip_day_hour'])\n",
    "        tr['cvr_gb_ip_app'] = GaussianSmoth().update_moment(tr['count_gb_ip_app'], tr['sum_label_gb_ip_app'])\n",
    "        tr['cvr_gb_ip_app_os'] = GaussianSmoth().update_moment(tr['count_gb_ip_app_os'], tr['sum_label_gb_ip_app_os'])\n",
    "\n",
    "        # Merge test fold with cvr features of train folds\n",
    "        te = te.merge(tr[['cvr_gb_ip_day_hour'] + idh].drop_duplicates(subset=idh, keep='first'), on=idh, how='left')\n",
    "        te = te.merge(tr[['cvr_gb_ip_app'] + ia].drop_duplicates(subset=ia, keep='first'), on=ia, how='left')\n",
    "        te = te.merge(tr[['cvr_gb_ip_app_os'] + iao].drop_duplicates(subset=iao, keep='first'), on=iao, how='left')\n",
    "\n",
    "        # Put it in train\n",
    "        train['cvr_gb_ip_day_hour'] += te['cvr_gb_ip_day_hour']\n",
    "        train['cvr_gb_ip_app'] += te['cvr_gb_ip_app']\n",
    "        train['cvr_gb_ip_app_os'] += te['cvr_gb_ip_app_os']\n",
    "\n",
    "        del tr, te\n",
    "        log('Fold ' + str(i) + ' Done!')\n",
    "\n",
    "    # Convert type\n",
    "    train['cvr_gb_ip_day_hour'] = train['cvr_gb_ip_day_hour'].astype(type)\n",
    "    train['cvr_gb_ip_app'] = train['cvr_gb_ip_app'].astype(type)\n",
    "    train['cvr_gb_ip_app_os'] = train['cvr_gb_ip_app_os'].astype(type)\n",
    "\n",
    "    # Merge cvr of train to test\n",
    "    test = test.merge(train[['cvr_gb_ip_day_hour'] + idh].drop_duplicates(subset=idh, keep='first'), on=idh, how='left')\n",
    "    test = test.merge(train[['cvr_gb_ip_app'] + ia].drop_duplicates(subset=ia, keep='first'), on=ia, how='left')\n",
    "    test = test.merge(train[['cvr_gb_ip_app_os'] + iao].drop_duplicates(subset=iao, keep='first'), on=iao, how='left')\n",
    "\n",
    "    predictors.append('cvr_gb_ip_day_hour')\n",
    "    predictors.append('cvr_gb_ip_app')\n",
    "    predictors.append('cvr_gb_ip_app_os')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Construct features function - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_local_train_test(df, train_size, test_size):\n",
    "    local_train = df[:train_size]\n",
    "    local_test = df[train_size:train_size + test_size]\n",
    "    return local_train, local_test\n",
    "\n",
    "\n",
    "def get_model_input_data(train, test, is_local):\n",
    "    feat = ['ip', 'app', 'device', 'os', 'channel', 'hour']\n",
    "    for f in feat:\n",
    "        if f not in predictors:\n",
    "            predictors.append(f)\n",
    "    train_x = train[predictors]\n",
    "    train_y = train.is_attributed.values\n",
    "    if is_local == 1:\n",
    "        test_x = test[train_x.columns.values]\n",
    "        test_y = test.is_attributed.values\n",
    "        return train_x, train_y, test_x, test_y\n",
    "    else:\n",
    "        test_x = test[train_x.columns.values]\n",
    "        return train_x, train_y, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, test_feature, test_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print(train_feature.columns)\n",
    "    params['scale_pos_weights'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=test_label)\n",
    "    num_round = rounds\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    print('XGBoost run cv: ' + 'round: ' + str(rounds))\n",
    "    res = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=20, early_stopping_rounds=50)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print('Time used:' + str(elapsed) + 's')\n",
    "    return res.best_ntree_limit, res.best_score, res\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=30)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv(train_feature, train_label, test_feature, test_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print(train_feature.columns)\n",
    "    params['scale_pos_weight'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    dtest = lgb.Dataset(test_feature, label=test_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    num_round = rounds\n",
    "    print('LightGBM run cv: ' + 'round: ' + str(rounds))\n",
    "    res = lgb.train(params, dtrain, num_round, valid_sets=[dtest], valid_names=['test'], verbose_eval=1, early_stopping_rounds=20)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print('Time used:', elapsed, 's')\n",
    "    return res.best_iteration, res.best_score['test']['auc'], res\n",
    "\n",
    "\n",
    "def lgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    num_round = rounds\n",
    "    model = lgb.train(params, dtrain, num_round, valid_sets=[dtrain], verbose_eval=1)\n",
    "    predict = model.predict(test_feature)\n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_result(test_index, pred, name):\n",
    "    result = pd.DataFrame({'click_id': test_index, 'is_attributed': pred})\n",
    "    result.to_csv(root_path + 'data/output/sub/' + name + '.csv', index=False, sep=',')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmoth(object):\n",
    "    def __init__(self, alpha=0, beta=0):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n",
    "        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n",
    "        print(self.alpha, self.beta)\n",
    "        return (self.alpha + success) / (self.alpha + self.beta + tries)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        # Cal mean and variance\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        mean = (success / tries).mean()\n",
    "        if len(tries) == 1:\n",
    "            var = 0\n",
    "        else:\n",
    "            var = (success / tries).var()\n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Read data ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:27 Read data...\n",
      "2019-04-23 17:32:29 Read data done!\n",
      "2019-04-23 17:32:29 Train data shape: (1000000, 7)\n",
      "2019-04-23 17:32:29 Test data shape: (1000000, 6)\n"
     ]
    }
   ],
   "source": [
    "log('Read data...')\n",
    "dtypes = {\n",
    "    'click_id': 'uint32',\n",
    "    'ip': 'uint32',\n",
    "    'app': 'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os': 'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8'\n",
    "}\n",
    "train = pd.read_csv( 'train.csv', header=0, sep=',', dtype=dtypes, usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed'],nrows=1000000 )\n",
    "test_supplement = pd.read_csv( 'test_supplement.csv', header=0, sep=',', dtype=dtypes, usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time'], nrows=1000000)\n",
    "gc.collect()\n",
    "log('Read data done!')\n",
    "log_shape(train, test_supplement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Preprocess ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:29 Process date...\n",
      "2019-04-23 17:32:35 Process date done!\n",
      "2019-04-23 17:32:35 Train data shape: (1000000, 16)\n",
      "2019-04-23 17:32:35 Test data shape: (1000000, 15)\n"
     ]
    }
   ],
   "source": [
    "log('Process date...')\n",
    "train = process_date(train)\n",
    "test_supplement = process_date(test_supplement)\n",
    "gc.collect()\n",
    "log('Process date done!')\n",
    "log_shape(train, test_supplement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Feature engineer ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:35 Train size:1000000\n"
     ]
    }
   ],
   "source": [
    "train_len = len(train)\n",
    "log('Train size:' + str(train_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:35 Train append test_supplement...\n",
      "2019-04-23 17:32:36 Train append test_supplement done!\n"
     ]
    }
   ],
   "source": [
    "log('Train append test_supplement...')\n",
    "df = train.append(test_supplement).reset_index(drop=True)\n",
    "del train\n",
    "del test_supplement\n",
    "gc.collect()\n",
    "log('Train append test_supplement done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:36 Before feature engineer\n",
      "2019-04-23 17:32:36 Num of features: 16\n",
      "2019-04-23 17:32:36 Features: Index(['app', 'channel', 'date', 'day', 'device', 'hour', 'ip',\n",
      "       'is_attributed', 'minute', 'month', 'os', 'second', 'tm_hour',\n",
      "       'tm_hour_cos', 'tm_hour_sin', 'weekday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "log('Before feature engineer')\n",
    "log('Num of features: ' + str(len(df.columns)))\n",
    "log('Features: ' + str(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:36 Cal next_time_delta\n",
      "2019-04-23 17:32:36 Calculate next_time_delta...\n",
      "2019-04-23 17:32:37 Calculate next_time_delta...\n",
      "2019-04-23 17:32:38 Calculate next_time_delta...\n",
      "2019-04-23 17:32:40 Cal prev_time_delta\n",
      "2019-04-23 17:32:40 Calculate prev_time_delta...\n",
      "2019-04-23 17:32:41 Calculate prev_time_delta...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log('Cal next_time_delta')\n",
    "df = cal_next_time_delta(df, 'next_time_delta', 'float32')\n",
    "gc.collect()\n",
    "log('Cal prev_time_delta')\n",
    "df = cal_prev_time_delta(df, 'prev_time_delta', 'float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:32:42 Cal nunique_channel_gb_ip\n",
      "2019-04-23 17:32:45 Cal nunique_app_gb_ip_device_os\n",
      "2019-04-23 17:32:50 Cal nunique_hour_gb_ip_day\n",
      "2019-04-23 17:32:53 Cal nunique_app_gb_ip\n",
      "2019-04-23 17:32:56 Cal nunique_os_gb_ip_app\n",
      "2019-04-23 17:33:01 Cal nunique_device_gb_ip\n",
      "2019-04-23 17:33:04 Cal nunique_channel_gb_app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log('Cal nunique_channel_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'channel', 'nunique_channel_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_app_gb_ip_device_os')\n",
    "df = merge_nunique(df, ['ip', 'device', 'os'], 'app', 'nunique_app_gb_ip_device_os', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_hour_gb_ip_day')\n",
    "df = merge_nunique(df, ['ip', 'day'], 'hour', 'nunique_hour_gb_ip_day', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_app_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'app', 'nunique_app_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_os_gb_ip_app')\n",
    "df = merge_nunique(df, ['ip', 'app'], 'os', 'nunique_os_gb_ip_app', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_device_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'device', 'nunique_device_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_channel_gb_app')\n",
    "df = merge_nunique(df, ['app'], 'channel', 'nunique_channel_gb_app', 'uint32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:07 Cal cumcount_os_gb_ip\n",
      "2019-04-23 17:33:08 Cal cumcount_app_gb_ip_device_os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log('Cal cumcount_os_gb_ip')\n",
    "df = merge_cumcount(df, ['ip'], 'os', 'cumcount_os_gb_ip', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal cumcount_app_gb_ip_device_os')\n",
    "df = merge_cumcount(df, ['ip', 'device', 'os'], 'app', 'cumcount_app_gb_ip_device_os', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:09 Cal count_gb_ip_day_hour\n",
      "2019-04-23 17:33:11 Cal count_gb_ip_app\n",
      "2019-04-23 17:33:14 Cal count_gb_ip_app_os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log('Cal count_gb_ip_day_hour')\n",
    "df = merge_count(df, ['ip', 'day', 'hour'], 'count_gb_ip_day_hour', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal count_gb_ip_app')\n",
    "df = merge_count(df, ['ip', 'app'], 'count_gb_ip_app', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal count_gb_ip_app_os')\n",
    "df = merge_count(df, ['ip', 'app', 'os'], 'count_gb_ip_app_os', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:18 Cal var_day_gb_ip_app_os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log('Cal var_day_gb_ip_app_os')\n",
    "df = merge_var(df, ['ip', 'app', 'os'], 'day', 'var_day_gb_ip_app_os', 'float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct features done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:22 After feature engineer\n",
      "2019-04-23 17:33:22 Num of features: 34\n",
      "2019-04-23 17:33:22 Features: Index(['app', 'channel', 'date', 'day', 'device', 'hour', 'ip',\n",
      "       'is_attributed', 'minute', 'month', 'os', 'second', 'tm_hour',\n",
      "       'tm_hour_cos', 'tm_hour_sin', 'weekday',\n",
      "       'ip_app_channel_device_os_next_time_delta',\n",
      "       'ip_os_device_next_time_delta', 'ip_os_device_app_next_time_delta',\n",
      "       'ip_channel_prev_time_delta', 'ip_os_prev_time_delta',\n",
      "       'nunique_channel_gb_ip', 'nunique_app_gb_ip_device_os',\n",
      "       'nunique_hour_gb_ip_day', 'nunique_app_gb_ip', 'nunique_os_gb_ip_app',\n",
      "       'nunique_device_gb_ip', 'nunique_channel_gb_app', 'cumcount_os_gb_ip',\n",
      "       'cumcount_app_gb_ip_device_os', 'count_gb_ip_day_hour',\n",
      "       'count_gb_ip_app', 'count_gb_ip_app_os', 'var_day_gb_ip_app_os'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "log('After feature engineer')\n",
    "log('Num of features: ' + str(len(df.columns)))\n",
    "log('Features: ' + str(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### All features save & reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all features\n",
    "pickle.dump(df, open('all.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload all features\n",
    "# df = cPickle.load(open(root_path + 'data/output/feat/all.p', 'rb'))\n",
    "# train_len = 184903891\n",
    "# dtypes = {\n",
    "#     'click_id': 'uint32',\n",
    "#     'ip': 'uint32',\n",
    "#     'app': 'uint16',\n",
    "#     'device': 'uint16',\n",
    "#     'os': 'uint16',\n",
    "#     'channel': 'uint16',\n",
    "#     'is_attributed': 'uint8'\n",
    "# }\n",
    "# predictors = ['ip', 'app', 'device', 'os', 'channel', 'hour',\n",
    "#               'next_time_delta', 'prev_time_delta',\n",
    "#               'nunique_channel_gb_ip', 'nunique_app_gb_ip_device_os',\n",
    "#               'nunique_hour_gb_ip_day', 'nunique_app_gb_ip', 'nunique_os_gb_ip_app',\n",
    "#               'nunique_device_gb_ip', 'nunique_channel_gb_app',\n",
    "#               'cumcount_os_gb_ip', 'cumcount_app_gb_ip_device_os',\n",
    "#               'count_gb_ip_day_hour', 'count_gb_ip_app', 'count_gb_ip_app_os',\n",
    "#               'var_day_gb_ip_app_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### All features save & reload - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:25 Train test_supplement divid...\n",
      "2019-04-23 17:33:25 Train data shape: (1000000, 34)\n",
      "2019-04-23 17:33:25 Test data shape: (1000000, 34)\n",
      "2019-04-23 17:33:25 Train test_supplement divid done!\n"
     ]
    }
   ],
   "source": [
    "log('Train test_supplement divid...')\n",
    "train = df[:train_len]\n",
    "test_supplement = df[train_len:]\n",
    "del df\n",
    "gc.collect()\n",
    "log_shape(train, test_supplement)\n",
    "log('Train test_supplement divid done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:25 Read test...\n",
      "2019-04-23 17:33:27 Test data original shape: (1000000, 7)\n"
     ]
    }
   ],
   "source": [
    "log('Read test...')\n",
    "test = pd.read_csv('test.csv', header=0, sep=',', dtype=dtypes, usecols=['click_id', 'ip', 'app', 'device', 'os', 'channel', 'click_time'], parse_dates=['click_time'],nrows=1000000)\n",
    "log('Test data original shape: ' + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:30 Train data shape: (1000000, 34)\n",
      "2019-04-23 17:33:30 Test data shape: (1000000, 35)\n",
      "2019-04-23 17:33:30 Read test done!\n"
     ]
    }
   ],
   "source": [
    "test = test.merge(test_supplement.drop_duplicates(subset=['ip', 'app', 'device', 'os', 'channel', 'date'], keep='first'), left_on=['ip', 'app', 'device', 'os', 'channel', 'click_time'], right_on=['ip', 'app', 'device', 'os', 'channel', 'date'], how='left')\n",
    "test.drop(['click_time'], axis=1, inplace=True)\n",
    "del test_supplement\n",
    "gc.collect()\n",
    "log_shape(train, test)\n",
    "log('Read test done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:33:30 Cal cvr...\n",
      "2019-04-23 17:33:30 Fold 0 begin...\n",
      "2019-04-23 17:33:30 Cal sum_label_gb_ip_day_hour\n",
      "2019-04-23 17:33:31 Cal sum_label_gb_ip_app\n",
      "2019-04-23 17:33:33 Cal sum_label_gb_ip_app_os\n",
      "0.002138335292281242 1.4507306562129352\n",
      "0.00037441349172921966 0.24676074136682433\n",
      "0.00031851350592673847 0.2007977547378011\n",
      "2019-04-23 17:33:36 Fold 0 Done!\n",
      "2019-04-23 17:33:36 Fold 1 begin...\n",
      "2019-04-23 17:33:37 Cal sum_label_gb_ip_day_hour\n",
      "2019-04-23 17:33:38 Cal sum_label_gb_ip_app\n",
      "2019-04-23 17:33:39 Cal sum_label_gb_ip_app_os\n",
      "0.0021517325982386756 1.4596803911856793\n",
      "0.0003722526276228888 0.24556140552068823\n",
      "0.0003029650914739995 0.19142063608413423\n",
      "2019-04-23 17:33:42 Fold 1 Done!\n",
      "2019-04-23 17:33:42 Fold 2 begin...\n",
      "2019-04-23 17:33:43 Cal sum_label_gb_ip_day_hour\n",
      "2019-04-23 17:33:44 Cal sum_label_gb_ip_app\n",
      "2019-04-23 17:33:45 Cal sum_label_gb_ip_app_os\n",
      "0.0021804954149106832 1.4632952845067138\n",
      "0.0003845161579382355 0.25030797183774706\n",
      "0.00031212443826963537 0.19458985730470296\n",
      "2019-04-23 17:33:49 Fold 2 Done!\n",
      "2019-04-23 17:33:49 Fold 3 begin...\n",
      "2019-04-23 17:33:49 Cal sum_label_gb_ip_day_hour\n",
      "2019-04-23 17:33:50 Cal sum_label_gb_ip_app\n",
      "2019-04-23 17:33:52 Cal sum_label_gb_ip_app_os\n",
      "0.0021658494919644987 1.4655047567412252\n",
      "0.0003953194553991607 0.2606032226972713\n",
      "0.00033050529176953364 0.20675845590982325\n",
      "2019-04-23 17:33:55 Fold 3 Done!\n",
      "2019-04-23 17:33:55 Fold 4 begin...\n",
      "2019-04-23 17:33:55 Cal sum_label_gb_ip_day_hour\n",
      "2019-04-23 17:33:57 Cal sum_label_gb_ip_app\n",
      "2019-04-23 17:33:58 Cal sum_label_gb_ip_app_os\n",
      "0.00206547112244514 1.3859142477724988\n",
      "0.0003512301647175174 0.2293489452512682\n",
      "0.00029162147924292393 0.18157547590737402\n",
      "2019-04-23 17:34:01 Fold 4 Done!\n",
      "2019-04-23 17:34:05 Cal cvr done!\n"
     ]
    }
   ],
   "source": [
    "# Cal cvr features\n",
    "log('Cal cvr...')\n",
    "train, test = cal_cvr(train, test, 'float32')\n",
    "log('Cal cvr done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CVR features save & reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr_feats = ['cvr_gb_ip_day_hour', 'cvr_gb_ip_app', 'cvr_gb_ip_app_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cvr features\n",
    "pickle.dump(train[cvr_feats], open('train_cvr.p', 'wb'))\n",
    "pickle.dump(test[cvr_feats], open('test_cvr.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload cvr features\n",
    "# train_cvr = cPickle.load(open(root_path + 'data/output/feat/train_cvr.p', 'rb'))\n",
    "# test_cvr = cPickle.load(open(root_path + 'data/output/feat/test_cvr.p', 'rb'))\n",
    "# train = pd.concat([train, train_cvr], axis=1)\n",
    "# test_cvr = pd.concat([test, test_cvr], axis=1)\n",
    "# del train_cvr, test_cvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CVR features save & reload - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Split dataset for local ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:34:05 Split dataset to get local train/test set...\n",
      "2019-04-23 17:34:05 Split dataset to get local train/test set done!\n",
      "2019-04-23 17:34:05 ================================= Local data info =====================================\n",
      "2019-04-23 17:34:05 Local train shape:(750000, 37)\n",
      "2019-04-23 17:34:05 Local test shape:(250000, 37)\n",
      "2019-04-23 17:34:05 Local train label ratio (0-1):[0.99827333 0.00172667]\n",
      "2019-04-23 17:34:05 Local train label number (0-1):[748705   1295]\n",
      "2019-04-23 17:34:05 Local train min/max date:2017-11-06 14:32:21,2017-11-06 16:15:49\n",
      "2019-04-23 17:34:05 Local test min/max date:2017-11-06 16:15:49,2017-11-06 16:21:51\n",
      "2019-04-23 17:34:05 =======================================================================================\n",
      "2019-04-23 17:34:05 ================================= Online data info =====================================\n",
      "2019-04-23 17:34:05 Online train shape:(1000000, 37)\n",
      "2019-04-23 17:34:05 Online test shape:(1000000, 38)\n",
      "2019-04-23 17:34:05 Online train label ratio (0-1):[0.998307 0.001693]\n",
      "2019-04-23 17:34:05 Online train label number (0-1):[998307   1693]\n",
      "2019-04-23 17:34:05 Online train min/max date:2017-11-06 14:32:21,2017-11-06 16:21:51\n",
      "2019-04-23 17:34:05 Online train min/max date:NaT,NaT\n",
      "2019-04-23 17:34:05 =======================================================================================\n"
     ]
    }
   ],
   "source": [
    "log('Split dataset to get local train/test set...')\n",
    "local_train_size = 750000  # 182403890\n",
    "local_test_size = 250000\n",
    "local_train, local_test = spilt_local_train_test(train, local_train_size, local_test_size)\n",
    "log('Split dataset to get local train/test set done!')\n",
    "\n",
    "log('================================= Local data info =====================================')\n",
    "log('Local train shape:' + str(local_train.shape))\n",
    "log('Local test shape:' + str(local_test.shape))\n",
    "log('Local train label ratio (0-1):' + str(local_train.is_attributed.value_counts().values * 1.0 / local_train.shape[0]))\n",
    "log('Local train label number (0-1):' + str(local_train.is_attributed.value_counts().values))\n",
    "log('Local train min/max date:' + str(local_train.date.min()) + ',' + str(local_train.date.max()))\n",
    "log('Local test min/max date:' + str(local_test.date.min()) + ',' + str(local_test.date.max()))\n",
    "log('=======================================================================================')\n",
    "\n",
    "log('================================= Online data info =====================================')\n",
    "log('Online train shape:' + str(train.shape))\n",
    "log('Online test shape:' + str(test.shape))\n",
    "log('Online train label ratio (0-1):' + str(train.is_attributed.value_counts().values * 1.0 / train.shape[0]))\n",
    "log('Online train label number (0-1):' + str(train.is_attributed.value_counts().values))\n",
    "log('Online train min/max date:' + str(train.date.min()) + ',' + str(train.date.max()))\n",
    "log('Online train min/max date:' + str(test.date.min()) + ',' + str(test.date.max()))\n",
    "log('=======================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:34:05 Get local model input data...\n",
      "2019-04-23 17:34:05 Train data shape: (750000, 27)\n",
      "2019-04-23 17:34:05 Test data shape: (250000, 27)\n",
      "2019-04-23 17:34:05 Get local model input data done!\n"
     ]
    }
   ],
   "source": [
    "log('Get local model input data...')\n",
    "local_train_x, local_train_y, local_test_x, local_test_y = get_model_input_data(local_train, local_test, is_local=1)\n",
    "del local_train\n",
    "del local_test\n",
    "gc.collect()\n",
    "log_shape(local_train_x, local_test_x)\n",
    "log('Get local model input data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 17:34:05 Get online model input data...\n",
      "2019-04-23 17:34:06 Train data shape: (1000000, 27)\n",
      "2019-04-23 17:34:06 Test data shape: (1000000, 27)\n",
      "2019-04-23 17:34:06 Get online model input data done!\n"
     ]
    }
   ],
   "source": [
    "log('Get online model input data...')\n",
    "online_train_x, online_train_y, online_test_x = get_model_input_data(train, test, is_local=0)\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "log_shape(online_train_x, online_test_x)\n",
    "log('Get online model input data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### LigthGBM ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_lgb = {\n",
    "    'rounds': 100,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params_lgb = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'xentropy',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.02,\n",
    "    # 'is_unbalance': 'true',  # Because training data is unbalance (replaced with scale_pos_weight)\n",
    "    'scale_pos_weight': 200,  # Because training data is extremely unbalanced\n",
    "    'num_leaves': 31,  # We should let it be smaller than 2^(max_depth)\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 128,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # Frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 0.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 0.9,  # L2 regularization term on weights\n",
    "    'nthread': 24,\n",
    "    'verbose': 1,\n",
    "    'seed': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 27)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ip_app_channel_device_os_next_time_delta',\n",
      "       'ip_os_device_next_time_delta', 'ip_os_device_app_next_time_delta',\n",
      "       'ip_channel_prev_time_delta', 'ip_os_prev_time_delta',\n",
      "       'nunique_channel_gb_ip', 'nunique_app_gb_ip_device_os',\n",
      "       'nunique_hour_gb_ip_day', 'nunique_app_gb_ip', 'nunique_os_gb_ip_app',\n",
      "       'nunique_device_gb_ip', 'nunique_channel_gb_app', 'cumcount_os_gb_ip',\n",
      "       'cumcount_app_gb_ip_device_os', 'count_gb_ip_day_hour',\n",
      "       'count_gb_ip_app', 'count_gb_ip_app_os', 'var_day_gb_ip_app_os',\n",
      "       'cvr_gb_ip_day_hour', 'cvr_gb_ip_app', 'cvr_gb_ip_app_os', 'ip', 'app',\n",
      "       'device', 'os', 'channel', 'hour'],\n",
      "      dtype='object')\n",
      "LightGBM run cv: round: 100\n",
      "[1]\ttest's auc: 0.956363\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttest's auc: 0.958833\n",
      "[3]\ttest's auc: 0.960584\n",
      "[4]\ttest's auc: 0.960882\n",
      "[5]\ttest's auc: 0.96105\n",
      "[6]\ttest's auc: 0.961396\n",
      "[7]\ttest's auc: 0.961133\n",
      "[8]\ttest's auc: 0.961427\n",
      "[9]\ttest's auc: 0.961505\n",
      "[10]\ttest's auc: 0.963073\n",
      "[11]\ttest's auc: 0.964011\n",
      "[12]\ttest's auc: 0.964228\n",
      "[13]\ttest's auc: 0.964121\n",
      "[14]\ttest's auc: 0.964355\n",
      "[15]\ttest's auc: 0.964317\n",
      "[16]\ttest's auc: 0.964865\n",
      "[17]\ttest's auc: 0.964936\n",
      "[18]\ttest's auc: 0.965312\n",
      "[19]\ttest's auc: 0.965269\n",
      "[20]\ttest's auc: 0.964997\n",
      "[21]\ttest's auc: 0.964957\n",
      "[22]\ttest's auc: 0.964806\n",
      "[23]\ttest's auc: 0.965062\n",
      "[24]\ttest's auc: 0.965276\n",
      "[25]\ttest's auc: 0.965297\n",
      "[26]\ttest's auc: 0.966114\n",
      "[27]\ttest's auc: 0.966276\n",
      "[28]\ttest's auc: 0.966278\n",
      "[29]\ttest's auc: 0.966267\n",
      "[30]\ttest's auc: 0.966314\n",
      "[31]\ttest's auc: 0.965896\n",
      "[32]\ttest's auc: 0.965986\n",
      "[33]\ttest's auc: 0.96587\n",
      "[34]\ttest's auc: 0.965989\n",
      "[35]\ttest's auc: 0.966028\n",
      "[36]\ttest's auc: 0.965832\n",
      "[37]\ttest's auc: 0.966595\n",
      "[38]\ttest's auc: 0.966794\n",
      "[39]\ttest's auc: 0.967185\n",
      "[40]\ttest's auc: 0.967046\n",
      "[41]\ttest's auc: 0.967086\n",
      "[42]\ttest's auc: 0.967148\n",
      "[43]\ttest's auc: 0.967147\n",
      "[44]\ttest's auc: 0.96707\n",
      "[45]\ttest's auc: 0.96716\n",
      "[46]\ttest's auc: 0.967097\n",
      "[47]\ttest's auc: 0.967127\n",
      "[48]\ttest's auc: 0.967607\n",
      "[49]\ttest's auc: 0.968204\n",
      "[50]\ttest's auc: 0.968019\n",
      "[51]\ttest's auc: 0.967737\n",
      "[52]\ttest's auc: 0.967875\n",
      "[53]\ttest's auc: 0.967722\n",
      "[54]\ttest's auc: 0.967699\n",
      "[55]\ttest's auc: 0.967647\n",
      "[56]\ttest's auc: 0.96762\n",
      "[57]\ttest's auc: 0.967603\n",
      "[58]\ttest's auc: 0.967585\n",
      "[59]\ttest's auc: 0.967661\n",
      "[60]\ttest's auc: 0.96749\n",
      "[61]\ttest's auc: 0.967206\n",
      "[62]\ttest's auc: 0.967218\n",
      "[63]\ttest's auc: 0.967014\n",
      "[64]\ttest's auc: 0.967186\n",
      "[65]\ttest's auc: 0.96717\n",
      "[66]\ttest's auc: 0.967244\n",
      "[67]\ttest's auc: 0.967327\n",
      "[68]\ttest's auc: 0.967284\n",
      "[69]\ttest's auc: 0.967409\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttest's auc: 0.968204\n",
      "Time used: 5.863839725672278 s\n"
     ]
    }
   ],
   "source": [
    "iterations_lgb, best_score_lgb, model_cv_lgb = lgb_cv(local_train_x, local_train_y, local_test_x, local_test_y, params_lgb, config_lgb['folds'], config_lgb['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_lgb = model_cv_lgb.predict(online_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.94116\n",
      "[2]\ttraining's auc: 0.957142\n",
      "[3]\ttraining's auc: 0.954473\n",
      "[4]\ttraining's auc: 0.956927\n",
      "[5]\ttraining's auc: 0.960503\n",
      "[6]\ttraining's auc: 0.960881\n",
      "[7]\ttraining's auc: 0.961142\n",
      "[8]\ttraining's auc: 0.961868\n",
      "[9]\ttraining's auc: 0.962767\n",
      "[10]\ttraining's auc: 0.962882\n",
      "[11]\ttraining's auc: 0.964428\n",
      "[12]\ttraining's auc: 0.965144\n",
      "[13]\ttraining's auc: 0.965172\n",
      "[14]\ttraining's auc: 0.965256\n",
      "[15]\ttraining's auc: 0.965376\n",
      "[16]\ttraining's auc: 0.965432\n",
      "[17]\ttraining's auc: 0.965752\n",
      "[18]\ttraining's auc: 0.965798\n",
      "[19]\ttraining's auc: 0.965698\n",
      "[20]\ttraining's auc: 0.966001\n",
      "[21]\ttraining's auc: 0.96599\n",
      "[22]\ttraining's auc: 0.965941\n",
      "[23]\ttraining's auc: 0.965949\n",
      "[24]\ttraining's auc: 0.966054\n",
      "[25]\ttraining's auc: 0.966583\n",
      "[26]\ttraining's auc: 0.966642\n",
      "[27]\ttraining's auc: 0.966787\n",
      "[28]\ttraining's auc: 0.96675\n",
      "[29]\ttraining's auc: 0.966945\n",
      "[30]\ttraining's auc: 0.966966\n",
      "[31]\ttraining's auc: 0.967203\n",
      "[32]\ttraining's auc: 0.967146\n",
      "[33]\ttraining's auc: 0.96721\n",
      "[34]\ttraining's auc: 0.967603\n",
      "[35]\ttraining's auc: 0.967311\n",
      "[36]\ttraining's auc: 0.967827\n",
      "[37]\ttraining's auc: 0.968189\n",
      "[38]\ttraining's auc: 0.96828\n",
      "[39]\ttraining's auc: 0.968564\n",
      "[40]\ttraining's auc: 0.968575\n",
      "[41]\ttraining's auc: 0.96916\n",
      "[42]\ttraining's auc: 0.969178\n",
      "[43]\ttraining's auc: 0.969689\n",
      "[44]\ttraining's auc: 0.969855\n",
      "[45]\ttraining's auc: 0.970063\n",
      "[46]\ttraining's auc: 0.970459\n",
      "[47]\ttraining's auc: 0.970561\n",
      "[48]\ttraining's auc: 0.97076\n",
      "[49]\ttraining's auc: 0.970709\n"
     ]
    }
   ],
   "source": [
    "model_lgb, pred_lgb = lgb_predict(online_train_x, online_train_y, online_test_x, iterations_lgb, params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_lgb = sorted(zip(online_train_x.columns, model_cv_lgb.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True)\n",
    "importance_lgb = pd.DataFrame({'feature': importance_lgb})\n",
    "importance_lgb = importance_lgb.apply(lambda x: pd.Series(x['feature']), axis=1)\n",
    "importance_lgb.columns = ['feature', 'importance']\n",
    "importance_lgb.to_csv('importance-lgb-20180507-%f(r%d).csv' % (best_score_lgb, iterations_lgb), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 1000000 does not match index length 18790469",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-de46845d819a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres_lgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'20180507-lgb-%f(r%d)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbest_score_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations_lgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-7f910dc71ff1>\u001b[0m in \u001b[0;36mstore_result\u001b[1;34m(test_index, pred, name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstore_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'click_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is_attributed'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'data/output/sub/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs542\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    391\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs542\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs542\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs542\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    326\u001b[0m                            \u001b[1;34m'length {idx_len}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                            .format(length=lengths[0], idx_len=len(index)))\n\u001b[1;32m--> 328\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array length 1000000 does not match index length 18790469"
     ]
    }
   ],
   "source": [
    "res_lgb = store_result(pd.read_csv('test.csv', header=0, sep=',', usecols=['click_id']).click_id.astype(int), pred_lgb, '20180507-lgb-%f(r%d)' % (best_score_lgb, iterations_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model save and reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "log('Save model...')\n",
    "model_lgb.save_model(root_path + 'data/output/model/lgb-%f(r%d).txt' % (best_score_lgb, iterations_lgb))\n",
    "log('Model best score:' + str(best_score_lgb))\n",
    "log('Model best iteration:' + str(iterations_lgb))\n",
    "log('Save model done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload model\n",
    "# model_lgb = lgb.Booster(model_file=root_path + 'data/output/model/lgb-0.981609(r2100).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model save and reload - end ###########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs542",
   "language": "python",
   "name": "cs542"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
